\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[T2A]{fontenc}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{ amssymb }
\usepackage{dsfont}
\usepackage{amsmath}



\newenvironment{sistema}%
{\left\lbrace\begin{array}{@{}l@{}}}%
{\end{array}\right.}

\geometry{
 a4paper,
 total={170mm,257mm},
 left=10mm,
 right=10mm,
 top=10mm,
 bottom=20mm
}

\begin{document}

\begin{center}
    \Huge{\textbf{Суммирование дискретных случайных величин \\[0.5in]}}
\end{center}



\begin{multicols}{2}
    \section{Инструкция к демонстрации}
    Пользователь может задать распределеление случайной величины, нажимая на график, и в соответствующей точке будет "вырастать" столбец-вероятность. Причем серая область на фоне графика отображает "свободную", ещё не распределенную вероятность. Ввод распределения завершается, когда сумма вероятностей заданных столбцов равна 1. Кроме того, есть возможность выбрать распределение из шаблонных. Есть возможность задать количество значений, принимаемых случайной величиной. Есть возможность дублировать первое распределение на втором графике. 

    Начинается процесс суммирования случайных величин, соответствующий формуле:
    \begin{equation}
        P_{\Sigma}(k) = \sum_{i=1}^{k}P_1(i) \cdot P_2(k-i)
    \end{equation}
    Гистограммы сближаются друг с другом, и когда столбцы с номерами $i_1$, $i_2$ совпадают, то на графике суммы появляется соответствующий столбец, соответствующий номеру $i_1 + i_2$. По итогу сумимрования формируется график суммы случайных величин. Есть возможность поставить демонстрацию на паузу (например, чтобы убедиться в верности построения графика суммы). Дополнительно по кнопке можно отобразить формулу для суммы распределений.

    Далее график суммы может быть использован для суммирования на следующем шаге. На новом шаге будет суммироваться случайная величина №1 и сумма случайных величин.

    Этот процесс можно повторять несколько раз, наблюдая, как сумма распределений приближается к нормальному.
    Для наглядности есть возможность по кнопке отобразить график нормального распределения. В правом верхнем углу  отображается номер шага, за счет чего можно установить, на каком шаге сумма достаточно близко приближается к нормальному. Дополнительно, по кноке можно перезапустить демонстрацию.
    
    \section{О сложении просто}
    Пусть мы имеем две конечных случайных дискретных величины, с распределениями вероятностей $\xi \thicksim P_\xi(n)$ и $\eta \thicksim P_\eta(m)$, где $n, m \in \mathds{N}$. Тогда посчитаем распределение суммы случайных величин. Для этого определим множество всех значений, принимаемых случайной величиной суммы: пусть $\xi$ принимает значения $A = \{a_1, a_2, ..., a_n\}$ и $\eta: B = \{b_1, b_2, ..., b_m\}$, тогда множество значений суммы представимо в виде $C = \bigcup\limits_{a\in A}\bigcup\limits_{b\in B}{\{a_i + b_j\}}$, причем вероятность каждого элемента равна $P_\xi(\xi = a_i)P_\eta(\eta = b_j)$. Однако в общем случае вариантов комбинаций значений случайных величин может быть несколько, а потому для получения полной вероятности для каждого значения суммы, нам потребуется провести операцию сложения следующего вида:
        $$P(\xi + \eta = c \in C) = \prod_{\substack{a + b = c \\ {a \in A, b \in B}}}{P_\xi(a)P_\eta(b)}=$$
        $$= \sum\limits_{i \in \mathds{N}}{P_\xi(i)P_\eta(c - i)} = 
        \sum\limits_{i \in \mathds{N}}{P_\xi(c - i)P_\eta(i)}$$

    \section{Повторение}
        Из курса теории вероятности вам уже известны три основных рода распределений, а из теоремы Лебега, вы знаете, что любое распределение случайной величины может быть разложено в выпуклую оболочку трех таких распределений. 

        Основополагающим и самым простым для понимания с точки зрения нашего жизненного опыта является именно дискретное распределение. Целью работы является наглядное представление операции сложения двух дискретных случайных величин.
        
        Введем все нужные для дальнейших выкладок определения и термины, а за одно вспомним начала курса Теории Вероятностей. Зададимся вопросом, когда именно возникает потребность в рассмотрении именно стохастической модели при анализе какого-либо процесса? Для этого наблюдаемый эксперимент должен быть воспроизводимым, а также в нем должен присутствовать эффект случайности. Введем основное понятие теории вероятностей — Колмогорову тройку в классическом виде $(\Omega, \mathcal{A}, P)$. Здесь $\Omega$ — множество всех исходов анализируемого эксперимента, где каждый элемент $\omega$ называется элементарным исходом. Будем рассматривать множество $\mathcal{A}$ как множество различных подмножеств множества $\Omega$, для которого выполняются условия на $\sigma$-алгебру. Над элементами множества $\mathcal{A}$ определим вероятностную меру $P: \mathcal{A} \longrightarrow \mathds{R}$, обладающую следующими свойствами
        \begin{itemize}
            \item $P(a \in \mathcal{A}) \geqslant 0$ \textit{-- неотрицательность}
            \item $P(\Omega) = 1$ \textit{-- нормировка}
            \item $\forall A_1, A_2, \ldots, A_n, \ldots \in \mathcal{A}, \\  A_i A_j=\varnothing(i \neq j): \mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i\right)=\sum_{i=1}^{\infty} \mathbb{P}\left(A_i\right)$ \textit{-- счетная аддитивность}
        \end{itemize}
        
        Тогда пару $(\Omega, \mathcal{A})$ будем называть измеримым пространством, определение которого состоит из множества и $\sigma$-алгебры над ним. Легко убедиться, что пара $(\mathds{R}, \mathcal{B})$, где $\mathcal{B}$ — Борелевская $\sigma$-алгебры, также является измеримым множеством. Определим понятие измеримой функции над такими двумя измеримыми пространствами: $\xi$ - измеримое отображение, если $\xi^{-1}(B)=\{\omega: \xi(\omega) \in B\} \in \mathcal{F} \quad \forall B \in \mathcal{B}$. Тогда измеримая функция $\xi: \Omega \longrightarrow \mathds{R}$ является по определению случайной величиной, действующей из $\Omega$ в $\mathds{R}$

        Как вы уже знаете, случайные величины удобны в использовании тем, что позволяют перейти только к их исследованию, даже если исследователь ничего не знает о начальном распределении (как это и бывает в задачах статистики). Действительно, определим на измеримом пространстве $(\mathds{R}, \mathcal{B})$ меру $P_{\xi}(b \in \mathcal{B}) = P({\xi^{-1}(\omega) \in b})=P(a \in \mathcal{A})$, где $a = \{\omega \in \Omega: \xi(\omega) \in b\}$. Получим новое вероятностное пространство, которое принято называть индуцированным случайной величиной $\xi$. 
        От нее же легко перейти к более удобному для нас в изучении понятию распределения, как функции $F_{\xi}(x)=P_{\xi}((-\inf; x) \in \mathcal{B}) = P(\xi \textless x)$. Легко убедиться, что выбор $(-\inf; x)$ не единственен, и вместо знака $\textless$ мог стоять любой другой знак неравенства, при любом выборе функция распределения однозначно определяет распределение случайной величины.

    \section{Переход к дискретным случайным величинам}
        Будем говорить, что множество $\mathcal{A}$ — счетно, или же что случайная величина принимает не более чем счетное количество значений. В таком случае случайная величина может быть представлена в виде набора пар значений/вероятностей их принять. Запишем это в виде таблицы 

        \begin{center}
            \begin{tabular}{ c|c|c|c|c|c } 
                 $\xi$ & $x_1$ & $x_2$ & $x_3$ & \ldots & $x_n$\\
                 \hline
                 $P(\xi = x_i)$ & $p_1$ & $p_2$ & $p_3$ & \ldots & $p_n$ \\
            \end{tabular}
        \end{center}

        Тогда легко представить вид функции распределения такой случайной величины: $F_{\xi}(x) = \sum_{x_i \textless x}p_i$, а саму случайную величину при разбиении начального множества $\Omega$ на совместно не пересекающиеся подмножества можно представить в виде суммы индикаторов: $$\xi = \sum_{a \in \mathcal{A}}{x_i \mathds{1}_{\{\xi(\omega) = x_i, \omega \in a\}}}, \hspace{0.1cm} a_ia_j = \varnothing, \hspace{0.1cm} i \neq j$$

\section{Независимые случайные величины}
    Определим понятие независимых событий: $A, B \in \mathcal{A}$ независимые события, если $P(AB)=P(A)P(B)$. Тогда будем говорить, что случайные величины $\xi$ и $\eta$ из одного измеримого пространства — независимы, если полные прообразы любых принимаемых значений величины $\xi$ независимы относительно полных прообразов величин, принимаемых $\eta$. 

\section{Математический аппарат,\\ стоящий за нашей демонстрацией}
    Теперь мы готовы описать аппарат, стоящий за сложением независимых дискретных величин. Определим случайную величину $\xi_1 = \{(x_1, p_1^1), \ldots (x_n, p_n^1)\}$ и $\xi_2 = \{(y_1, p_1^2), \ldots (y_n, p_m^2)\}$. Тогда определим новую случайную величину $\eta = \xi_1 + \xi_2$ и посмотрим, как будет выглядеть распределение такой случайной величины. Построим такое непересекающееся разбиение $\Omega$, что:

    $$\eta(\omega) = \sum_{a \in \mathcal{A}}{(x_i + y_j) \mathds{1}_{\{\xi_1(\omega) = x_i, \xi_2(\omega) = y_j \hspace{0.05cm} | \hspace{0.05cm} \omega \in a\}}}$$

    Тогда вероятность каждого из значений случайной величины $\eta$ будет исходить из вероятности принятия значения 1 индикатором $\mathds{1}_{\{\xi_1(\omega) = x_i, \xi_2(\omega) = y_j \hspace{0.05cm} | \hspace{0.05cm} \omega \in a\}}$. Найдем такую вероятность, при определении индикатора:
    \[
      \mathds{1}_{A}(\omega) =
      \begin{sistema}
        1, \hspace{0.1cm} \omega \in A\\
        0, \hspace{0.1cm} \omega \notin A 
      \end{sistema} 
    \]
    Тогда $P(\mathds{1}_{\{\xi_1(\omega) = x_i, \xi_2(\omega) = y_j \hspace{0.05cm} | \hspace{0.05cm} \omega \in a\}} = 1) =\\ 
    P(\xi_1(\omega) = x_i, \xi_2(\omega) = y_j)= P(\xi_1^{-1}(x_i), \xi_2^{-1}(y_j))=\\ 
    P(A \in \mathcal{A}, B \in \mathcal{A})$, где  $A$ полный прообраз $\xi_1^{-1}(x_i)$, а $B$ полный прообраз соответственно для $\xi_2^{-1}(y_j)$. Вернемся к условию независимости двух случайных величин. Получим, что любые прообразы $\xi_1$ и $\xi_2$ -- независимые события, а значит, мы имеем право расписать получившееся равенство как:
    $$P(\mathds{1}_{\{\xi_1(\omega) = x_i, \xi_2(\omega) = y_j | \hspace{0.05cm} \omega \in a\}} = 1) = P(\xi_1(\omega) = x_i)P(\xi_2(\omega) = y_j)$$

    Поэтому исходная сумма может быть переписана в виде:
    $$\eta(\omega) = \sum_{a \in \mathcal{A}}{(x_i + y_j) \mathds{1}_{\{\xi_1(\omega) = x_i | \hspace{0.05cm} \omega \in a\}} \mathds{1}_{\{\xi_2(\omega) = y_j | \hspace{0.05cm} \omega \in a\}}}$$
    Причем суммирование проводится по такому разбиению начального множества $\Omega$, что каждое подмножество множество $a \in \mathcal{A}$, соответсвующее $i/j$  значениям, не пересекается с другими множествами из разбиения, а сама $\eta$ может быть записана:
    \begin{center}
        \begin{tabular}{ c|c|c|c|c|c } 
             $\eta$ & $x_1 + y_1$ & $x_2 + y_1$ & \ldots & $x_n + y_{m-1}$ & $x_n + y_m$\\
             \hline
             $P(\eta = x)$ & $p^1_1 \times p^2_1$ & $p^1_2 \times p^2_1$ & \ldots & $p^1_n \times p^2_{m-1}$ & $p^1_n \times p^2_m$\\
        \end{tabular}
    \end{center}

    Наша демонстрация призвана наглядно показать, как рождается распределение суммы случайных величин в предположении, что все случайные величины, входящие в ряд, независимы в совокупности. На деле же редко приходится говорить не только о независимости в совокупности случайных величин из одного измеримого пространства, но и о попарной независимости случайных величин. 

\section{Больше о сложении случайных величин}
    В общем случае нельзя утверждать, что операция сложения двух случайных величин даже из одного распределения не выводит результирующую величину из класса данного распределения. Если же случайная величина может быть представлена как сумма двух случайных величин, имеющих такой же класс распределений, что и результирующая случайная величина, то такая величина называется делимой. Соответственно, если операция сложения не выводит за рамки класса распределения, то такое распределение называется \textbf{бесконечно делимым}.

    Таковыми, например, являются распределение Коши, Пуассона, Нормальное и Гамма распределение. Из теоремы Колмогорова следует критерий безграничной делимости распределения: Для того, чтобы функция распределения $\Phi(x)$ с конечной дисперсией была бесконечно делимой, необходимо и достаточно, чтобы логарифм её характеристической функции $\varphi(t)$ имел вид: 

    $$\ln \varphi(t)=i \gamma t+\int_{-\infty}^{\infty} \frac{e^{i t x}-1-i t x}{x^2} d G(x)$$

\section{Рекомендуемая литература}
    Литература, описанная ниже не только использовалась при написании теории работы, но и рекомендуется к прочтению всем, кто хочет углубить свои знания в теории вероятности и научиться применять ее на практике:

    \begin{itemize}
        \item А.Н. Ширяев — "Вероятность" в двух томах — \textbf{мой личный выбор}.
        \item Феллер - "Введение в курс вероятностей и ее приложения".
        \item Гнеденко - "Курс теории вероятности".
        \item Курс лекций по теории вероятностей Ульянова В.С. 
        \item Ивченко, Медведев - "Введение в мат. статистику".
    \end{itemize}
\end{multicols}

\begin{center}
    Благодарим за ваше внимание и интерес к науке теории вероятности.
    
    Подготовили Федоров Артем и Левыкин Александр.
    
    3 курс. ВМК. 2023 год
\end{center}

\end{document}
